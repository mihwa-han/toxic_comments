{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ogrellier/lgbm-with-words-and-chars-n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-12T02:02:42.998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reading input files] done in 4 s\n",
      "[Performing basic NLP] done in 456 s\n",
      "[Creating numerical features] done in 1 s\n",
      "[Tfidf on word] done in 193 s\n",
      "[Tfidf on char n_gram] done in 317 s\n",
      "1336\n",
      "[Staking matrices] done in 37 s\n",
      "Class toxic scores : \n",
      "\t Fold 1 : 0.977182 in 500 rounds\n",
      "\t Fold 2 : 0.976587 in 500 rounds\n",
      "\t Fold 3 : 0.975157 in 500 rounds\n",
      "\t Fold 4 : 0.977660 in 376 rounds\n",
      "full score : 0.976423\n",
      "Class severe_toxic scores : \n",
      "\t Fold 1 : 0.988026 in  85 rounds\n",
      "\t Fold 2 : 0.986492 in 118 rounds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "\n",
    "def get_indicators_and_clean_comments(df):\n",
    "    \"\"\"\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "    Though I'm not sure all of them improve scores\n",
    "    \"\"\"\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    # Get length in words and characters\n",
    "    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n",
    "    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "    # Number of different characters used in a comment\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "\n",
    "\n",
    "\n",
    "def char_analyzer(text):\n",
    "    \"\"\"\n",
    "    This is used to split strings in small lots\n",
    "    I saw this in an article (I can't find the link anymore)\n",
    "    so <talk> and <talking> would have <Tal> <alk> in common\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gc.enable()\n",
    "    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "    with timer(\"Reading input files\"):\n",
    "        train = pd.read_csv('../input/train.csv').fillna(' ')\n",
    "        test = pd.read_csv('../input/test.csv').fillna(' ')\n",
    "\n",
    "    with timer(\"Performing basic NLP\"):\n",
    "        get_indicators_and_clean_comments(train)\n",
    "        get_indicators_and_clean_comments(test)\n",
    "\n",
    "\n",
    "    # Scaling numerical features with MinMaxScaler though tree boosters don't need that\n",
    "    with timer(\"Creating numerical features\"):\n",
    "        num_features = [f_ for f_ in train.columns\n",
    "                        if f_ not in [\"comment_text\", \"clean_comment\", \"id\", \"remaining_chars\",\n",
    "                                      'has_ip_address'] + class_names]\n",
    "\n",
    "        skl = MinMaxScaler()\n",
    "        train_num_features = csr_matrix(skl.fit_transform(train[num_features]))\n",
    "        test_num_features = csr_matrix(skl.fit_transform(test[num_features]))\n",
    "\n",
    "    # Get TF-IDF features\n",
    "    train_text = train['clean_comment']\n",
    "    test_text = test['clean_comment']\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "    # First on real words\n",
    "    with timer(\"Tfidf on word\"):\n",
    "        word_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}',\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=20000)\n",
    "        word_vectorizer.fit(all_text)\n",
    "        train_word_features = word_vectorizer.transform(train_text)\n",
    "        test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "    del word_vectorizer\n",
    "    gc.collect()\n",
    "\n",
    "    # Now use the char_analyzer to get another TFIDF\n",
    "    # Char level TFIDF would go through words when char analyzer only considers\n",
    "    # characters inside a word\n",
    "    with timer(\"Tfidf on char n_gram\"):\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            sublinear_tf=True,\n",
    "            strip_accents='unicode',\n",
    "            tokenizer=char_analyzer,\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=50000)\n",
    "        char_vectorizer.fit(all_text)\n",
    "        train_char_features = char_vectorizer.transform(train_text)\n",
    "        test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "    del char_vectorizer\n",
    "    gc.collect()\n",
    "\n",
    "    print((train_char_features > 0).sum(axis=1).max())\n",
    "\n",
    "    del train_text\n",
    "    del test_text\n",
    "    gc.collect()\n",
    "\n",
    "    # Now stack TF IDF matrices\n",
    "    with timer(\"Staking matrices\"):\n",
    "        csr_trn = hstack(\n",
    "            [\n",
    "                train_char_features,\n",
    "                train_word_features,\n",
    "                train_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        # del train_word_features\n",
    "        del train_num_features\n",
    "        del train_char_features\n",
    "        gc.collect()\n",
    "\n",
    "        csr_sub = hstack(\n",
    "            [\n",
    "                test_char_features,\n",
    "                test_word_features,\n",
    "                test_num_features\n",
    "            ]\n",
    "        ).tocsr()\n",
    "        # del test_word_features\n",
    "        del test_num_features\n",
    "        del test_char_features\n",
    "        gc.collect()\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    # Drop now useless columns in train and test\n",
    "    drop_f = [f_ for f_ in train if f_ not in [\"id\"] + class_names]\n",
    "    train.drop(drop_f, axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "\n",
    "    # Set LGBM parameters\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        'metric': {'auc'},\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "        \"num_threads\": 4,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"num_leaves\": 31,\n",
    "        \"verbose\": -1,\n",
    "        \"min_split_gain\": .1,\n",
    "        \"reg_alpha\": .1\n",
    "    }\n",
    "\n",
    "    # Now go through folds\n",
    "    # I use K-Fold for reasons described here : \n",
    "    # https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/49964\n",
    "    with timer(\"Scoring Light GBM\"):\n",
    "        scores = []\n",
    "        folds = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "        lgb_round_dict = defaultdict(int)\n",
    "        trn_lgbset = lgb.Dataset(csr_trn, free_raw_data=False)\n",
    "        del csr_trn\n",
    "        gc.collect()\n",
    "        \n",
    "        for class_name in class_names:\n",
    "            print(\"Class %s scores : \" % class_name)\n",
    "            class_pred = np.zeros(len(train))\n",
    "            train_target = train[class_name]\n",
    "            trn_lgbset.set_label(train_target.values)\n",
    "            \n",
    "            lgb_rounds = 500\n",
    "\n",
    "            for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, train_target)):\n",
    "                watchlist = [\n",
    "                    trn_lgbset.subset(trn_idx),\n",
    "                    trn_lgbset.subset(val_idx)\n",
    "                ]\n",
    "                # Train lgb l1\n",
    "                model = lgb.train(\n",
    "                    params=params,\n",
    "                    train_set=watchlist[0],\n",
    "                    num_boost_round=lgb_rounds,\n",
    "                    valid_sets=watchlist,\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=0\n",
    "                )\n",
    "                class_pred[val_idx] = model.predict(trn_lgbset.data[val_idx], num_iteration=model.best_iteration)\n",
    "                score = roc_auc_score(train_target.values[val_idx], class_pred[val_idx])\n",
    "                \n",
    "                # Compute mean rounds over folds for each class\n",
    "                # So that it can be re-used for test predictions\n",
    "                lgb_round_dict[class_name] += model.best_iteration\n",
    "                print(\"\\t Fold %d : %.6f in %3d rounds\" % (n_fold + 1, score, model.best_iteration))\n",
    "            \n",
    "            print(\"full score : %.6f\" % roc_auc_score(train_target, class_pred))\n",
    "            scores.append(roc_auc_score(train_target, class_pred))\n",
    "            train[class_name + \"_oof\"] = class_pred\n",
    "\n",
    "        # Save OOF predictions - may be interesting for stacking...\n",
    "        train[[\"id\"] + class_names + [f + \"_oof\" for f in class_names]].to_csv(\"lvl0_lgbm_clean_oof.csv\",\n",
    "                                                                               index=False,\n",
    "                                                                               float_format=\"%.8f\")\n",
    "\n",
    "        print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "    with timer(\"Predicting probabilities\"):\n",
    "        # Go through all classes and reuse computed number of rounds for each class\n",
    "        for class_name in class_names:\n",
    "            with timer(\"Predicting probabilities for %s\" % class_name):\n",
    "                train_target = train[class_name]\n",
    "                trn_lgbset.set_label(train_target.values)\n",
    "                # Train lgb\n",
    "                model = lgb.train(\n",
    "                    params=params,\n",
    "                    train_set=trn_lgbset,\n",
    "                    num_boost_round=int(lgb_round_dict[class_name] / folds.n_splits)\n",
    "                )\n",
    "                submission[class_name] = model.predict(csr_sub, num_iteration=model.best_iteration)\n",
    "\n",
    "submission.to_csv(\"submission_lvl0_lgbm_clean_sub.csv\", index=False, float_format=\"%.8f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
