{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/michaelsnell/conv1d-dpcnn-in-keras\n",
    "\n",
    "850 views\n",
    "27 voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T02:08:52.237607Z",
     "start_time": "2018-03-13T23:04:16.587512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing start\n",
      "preprocessing done\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/4\n",
      " - 3610s - loss: 0.0728 - acc: 0.9745 - val_loss: 0.0580 - val_acc: 0.9806\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.973889 \n",
      "\n",
      "Epoch 2/4\n",
      " - 2451s - loss: 0.0498 - acc: 0.9817 - val_loss: 0.0494 - val_acc: 0.9815\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.979234 \n",
      "\n",
      "Epoch 3/4\n",
      " - 1961s - loss: 0.0465 - acc: 0.9824 - val_loss: 0.0459 - val_acc: 0.9826\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.982976 \n",
      "\n",
      "Epoch 4/4\n",
      " - 1238s - loss: 0.0452 - acc: 0.9827 - val_loss: 0.0454 - val_acc: 0.9826\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.983602 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dpcnn http://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf\n",
    "#dpcnn with conv1d, model architecture and all parameters copied from neptune-ml since it's publicly available\n",
    "#https://github.com/neptune-ml/kaggle-toxic-starter/blob/master/best_configs/fasttext_dpcnn.yaml\n",
    "#Got it to PLB 0.984 with 10fold cv on local computer after playing with parameters\n",
    "#Try to improve score on your own local pc or throw it in the blender with the rest of them :)\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\n",
    "from keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras import initializers, regularizers, constraints, callbacks\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "def schedule(ind):\n",
    "    a = [0.001, 0.0005, 0.0001, 0.0001]\n",
    "    return a[ind] \n",
    "\n",
    "#straightfoward preprocess\n",
    "\n",
    "EMBEDDING_FILE = '../input/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "\n",
    "           \n",
    "print('preprocessing start')\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "del all_embs, X_train, X_test, train, test\n",
    "gc.collect()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "print('preprocessing done')\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4)\n",
    "K.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n",
    "\n",
    "#model\n",
    "#wrote out all the blocks instead of looping for simplicity\n",
    "filter_nr = 64\n",
    "filter_size = 3\n",
    "max_pool_size = 3\n",
    "max_pool_strides = 2\n",
    "dense_nr = 256\n",
    "spatial_dropout = 0.2\n",
    "dense_dropout = 0.5\n",
    "train_embed = False\n",
    "\n",
    "comment = Input(shape=(maxlen,))\n",
    "emb_comment = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(comment)\n",
    "emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n",
    "\n",
    "block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = PReLU()(block1)\n",
    "block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = PReLU()(block1)\n",
    "\n",
    "#we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "#if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment)\n",
    "resize_emb = PReLU()(resize_emb)\n",
    "    \n",
    "block1_output = add([block1, resize_emb])\n",
    "block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "\n",
    "block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = PReLU()(block2)\n",
    "block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = PReLU()(block2)\n",
    "    \n",
    "block2_output = add([block2, block1_output])\n",
    "block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "\n",
    "block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = PReLU()(block3)\n",
    "block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = PReLU()(block3)\n",
    "    \n",
    "block3_output = add([block3, block2_output])\n",
    "block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n",
    "\n",
    "block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output)\n",
    "block4 = BatchNormalization()(block4)\n",
    "block4 = PReLU()(block4)\n",
    "block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block4)\n",
    "block4 = BatchNormalization()(block4)\n",
    "block4 = PReLU()(block4)\n",
    "\n",
    "output = add([block4, block3_output])\n",
    "output = GlobalMaxPooling1D()(output)\n",
    "output = Dense(dense_nr, activation='linear')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = PReLU()(output)\n",
    "output = Dropout(dense_dropout)(output)\n",
    "output = Dense(6, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(comment, output)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "            optimizer=optimizers.Adam(),\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "batch_size = 128\n",
    "epochs = 4\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "\n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "ra_val = RocAucEvaluation(validation_data=(Xval, yval), interval = 1)\n",
    "model.fit(Xtrain, ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, yval), callbacks = [lr, ra_val] ,verbose=2)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission_dpcnn_test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
